{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZGwkEenoJYH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import json\n",
        "import yaml\n",
        "\n",
        "# Ensure output directories exist\n",
        "os.makedirs(\"figures\", exist_ok=True)\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "# Project utilities fallback\n",
        "try:\n",
        "    from src.models import MLP\n",
        "    from src.metrics import OptimizationLogger\n",
        "    from src.reproducibility import set_seed\n",
        "    from src.utils import load_config\n",
        "    print(\"Successfully imported from src library.\")\n",
        "except ImportError:\n",
        "    print(\"src/ not found. Defining core components locally for standalone execution.\")\n",
        "    \n",
        "    def set_seed(seed=42):\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    def load_config(path):\n",
        "        if os.path.exists(path):\n",
        "            with open(path, 'r') as f:\n",
        "                return yaml.safe_load(f)\n",
        "        return {\n",
        "            \"training\": {\"batch_size\": 64, \"epochs\": 10},\n",
        "            \"seeds\": [0, 1, 2, 3, 4],\n",
        "            \"model\": {\"input_size\": 784, \"hidden_size\": 128, \"output_size\": 10},\n",
        "            \"optimizers\": {\n",
        "                \"adam\": {\"lr\": 0.001, \"weight_decay\": 0.0001},\n",
        "                \"adamw\": {\"lr\": 0.001, \"weight_decay\": 0.0001}\n",
        "            }\n",
        "        }\n",
        "\n",
        "    class MLP(nn.Module):\n",
        "        def __init__(self, input_size=784, hidden_size=128, output_size=10):\n",
        "            super().__init__()\n",
        "            self.net = nn.Sequential(\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(input_size, hidden_size),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_size, output_size)\n",
        "            )\n",
        "        def forward(self, x):\n",
        "            return self.net(x)\n",
        "\n",
        "    class OptimizationLogger:\n",
        "        def __init__(self, name=\"Experiment\"):\n",
        "            self.name = name\n",
        "            self.logs = {\"iteration\": [], \"objective_value\": [], \"gradient_norm\": [], \"elapsed_time\": []}\n",
        "            self.start_time = time.perf_counter()\n",
        "        def get_summary(self):\n",
        "            return self.logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxMqv4Dv0cK7",
        "outputId": "68d36607-6875-4bc9-d197-c9d823550822"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration Management\n",
        "\n",
        "All hyperparameters (model size, optimizer settings, batch size, epochs, and seeds)\n",
        "are loaded from an external YAML file to ensure reproducibility and fair benchmarking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cfg = load_config(\"configs/nn.yaml\")\n",
        "\n",
        "batch_size = cfg.get(\"training\", {}).get(\"batch_size\", 64)\n",
        "epochs = cfg.get(\"training\", {}).get(\"epochs\", 10)\n",
        "seeds = cfg.get(\"seeds\", [0, 1, 2, 3, 4])\n",
        "\n",
        "adam_cfg = cfg.get(\"optimizers\", {}).get(\"adam\", {})\n",
        "adamw_cfg = cfg.get(\"optimizers\", {}).get(\"adamw\", {})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "xEjSngY8pXU5"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "7Vlh8MU9pX9H"
      },
      "outputs": [],
      "source": [
        "train_ds = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_ds  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6mFcWe5pgQa"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNKNAxUop8sx"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            preds = model(x).argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_grad_norm(model):\n",
        "    total_norm = 0.0\n",
        "    for p in model.parameters():\n",
        "        if p.grad is not None:\n",
        "            param_norm = p.grad.data.norm(2)\n",
        "            total_norm += param_norm.item() ** 2\n",
        "    return total_norm ** 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcYHKXb5q99E"
      },
      "outputs": [],
      "source": [
        "def run_experiment(optimizer_name, optimizer_cfg, seed):\n",
        "    # Reproducibility\n",
        "    set_seed(seed)\n",
        "\n",
        "    # Model\n",
        "    model = MLP(**cfg.get(\"model\", {\"hidden_size\": 128})).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer\n",
        "    if optimizer_name == \"Adam\":\n",
        "        optimizer = optim.Adam(\n",
        "            model.parameters(),\n",
        "            lr=optimizer_cfg.get(\"lr\", 1e-3),\n",
        "            weight_decay=optimizer_cfg.get(\"weight_decay\", 1e-4)\n",
        "        )\n",
        "    else:\n",
        "        optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=optimizer_cfg.get(\"lr\", 1e-3),\n",
        "            weight_decay=optimizer_cfg.get(\"weight_decay\", 1e-4)\n",
        "        )\n",
        "\n",
        "    # Logger\n",
        "    logger = OptimizationLogger(name=f\"{optimizer_name}_seed_{seed}\")\n",
        "    logger.logs[\"test_accuracy\"] = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        epoch_grad_norms = []\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            loss.backward()\n",
        "\n",
        "            epoch_grad_norms.append(compute_grad_norm(model))\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        epoch_loss /= len(train_loader)\n",
        "        mean_grad_norm = float(np.mean(epoch_grad_norms))\n",
        "        test_acc = evaluate(model, test_loader)\n",
        "\n",
        "        logger.logs[\"iteration\"].append(epoch)\n",
        "        logger.logs[\"objective_value\"].append(epoch_loss)\n",
        "        logger.logs[\"gradient_norm\"].append(mean_grad_norm)\n",
        "        logger.logs[\"test_accuracy\"].append(test_acc)\n",
        "        logger.logs[\"elapsed_time\"].append(time.perf_counter() - logger.start_time)\n",
        "\n",
        "        print(f\"{optimizer_name} | seed {seed} | epoch {epoch+1}/{epochs} | loss={epoch_loss:.4f} | acc={test_acc:.4f}\")\n",
        "\n",
        "    return logger.get_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oEP9iVyrGk4",
        "outputId": "4c5d1a3f-3c75-4dcd-8be6-47b58e43c082"
      },
      "outputs": [],
      "source": [
        "adam_results = []\n",
        "adamw_results = []\n",
        "\n",
        "for s in seeds:\n",
        "    print(f\"Running Adam with seed {s}...\")\n",
        "    adam_results.append(run_experiment(\"Adam\", adam_cfg, s))\n",
        "    print(f\"Running AdamW with seed {s}...\")\n",
        "    adamw_results.append(run_experiment(\"AdamW\", adamw_cfg, s))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adam vs AdamW: Weight Decay in Adaptive Methods\n",
        "\n",
        "Standard Adam implements regularization as L2 penalty added to the gradient, which\n",
        "interacts incorrectly with adaptive learning rates. AdamW decouples weight decay from\n",
        "the gradient update, applying it directly to the parameters.\n",
        "\n",
        "Loshchilov and Hutter (2019) showed that AdamW restores the intended behavior of weight\n",
        "decay, leading to improved generalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize(results):\n",
        "    final_losses = [r[\"objective_value\"][-1] for r in results]\n",
        "    return np.mean(final_losses), np.std(final_losses)\n",
        "\n",
        "adam_mean, adam_std = summarize(adam_results)\n",
        "adamw_mean, adamw_std = summarize(adamw_results)\n",
        "\n",
        "print(f\"Adam   Final Loss: {adam_mean:.4f} ± {adam_std:.4f}\")\n",
        "print(f\"AdamW  Final Loss: {adamw_mean:.4f} ± {adamw_std:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "for r in adam_results:\n",
        "    plt.plot(r[\"objective_value\"], color=\"red\", alpha=0.3, label=\"Adam\" if r == adam_results[0] else \"\")\n",
        "for r in adamw_results:\n",
        "    plt.plot(r[\"objective_value\"], color=\"blue\", alpha=0.3, label=\"AdamW\" if r == adamw_results[0] else \"\")\n",
        "\n",
        "plt.title(\"Training Loss: Adam (red) vs AdamW (blue)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.savefig(\"figures/nn_training_curves.png\")\n",
        "plt.show()\n",
        "\n",
        "# Final result saving\n",
        "final_data = {\n",
        "    \"adam_losses\": [r[\"objective_value\"][-1] for r in adam_results],\n",
        "    \"adamw_losses\": [r[\"objective_value\"][-1] for r in adamw_results],\n",
        "    \"adam_accs\": [r[\"test_accuracy\"][-1] for r in adam_results],\n",
        "    \"adamw_accs\": [r[\"test_accuracy\"][-1] for r in adamw_results]\n",
        "}\n",
        "with open(\"results/nn_results.json\", \"w\") as f:\n",
        "    json.dump(final_data, f, indent=4)\n",
        "print(\"Results saved to results/nn_results.json\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}